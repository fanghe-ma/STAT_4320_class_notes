\section{Class 8}

\subsection{Maximum Likelihood Estimation}

\begin{definition}
    (Maximum Likelihood Estimation): Let $X_1, \hdots X_n$ iid from $F$ with \textit{pdf} or \textit{pmf} $ f_{ \theta}$. Consider the joint \textit{pdf} or \textit{pmf} 
    \[
        L ( \theta | x_1, x_2, \hdots x_n) = \prod_{i = 1}^n f_{ \theta} (x_i)
    \]
    The maximum likelihood estimator is given by 
    \[
        \hat{ \theta}_n = \arg \max_{ \Theta} L ( \theta | x_1, x_2, \hdots x_n)
    \]
    Equivalently, define 
    \begin{align*}
        l( \theta | x_1, \hdots x_n) &:= \log L( \theta | x_1, \hdots x_n) \\
        &= \sum\limits_{i = 1}^{n}  \log f_{ \theta} (x_i)
    \end{align*}
    \[
        \hat{ \theta}_n = \arg \max_{ \Theta} l ( \theta | x_1, x_2, \hdots x_n)
    \]
\end{definition} 

\begin{example}
(Bernoulli) $X_1, \hdots X_n \sim Ber(p)$. 

\begin{align*}
    f_p(x) &= P(X = x) \\
    &= \begin{cases}
        p \text{ if } x = 1 \\
        1 - p \text{ if } x = 0 \\
    \end{cases} \\
    &= p^x (1-p)^{1-x}
\end{align*}

The likelihood function is 
\begin{align*}
    L( p | X_1, \hdots X_n) &= \prod_{i =1}^n f_p(X_i) \\
    &= \prod_{i = 1}^n p^{X_i} (1-p)^{1-X_i} \\
    &= p^{ \sum\limits_{i = 1}^{n} X_i} (1-p)^{n - \sum\limits_{i = 1}^{n}X_i } \\
    l(p |X_1, X_2, \hdots X_n) &= \left( \sum\limits_{i = 1}^{n} X_i \right)  \log p + \left( n - \sum\limits_{i = 1}^{n} X_i \right)  \log (1-p)
\end{align*}

Define $T = \sum\limits_{i = 1}^{n}X_i $, 
\begin{align*}
    l(p | X_1, \hdots X_n) &= T \log p + (n - T) \log (1-p) \\
    \frac{d}{dp} l(p | X_1, \hdots X_n) &= \frac{T}{p} - \frac{n-T}{1-p} \\
    &= 0
\end{align*}

Hence 
\[
    \hat{p}_{MLE} = \frac{T}{n} = \frac{1}{n} \sum\limits_{i = 1}^{n}  X_i
\]

To show that this is indeed a maximum, 
\[
    \frac{d^2}{d^2p} l(p | X_1, \hdots X_n) = - \frac{T}{p^2} - \frac{T}{(1-p)^2} < 0
\]

Hence $l(p|X_1, \hdots X_n)$ is concave and $ \hat{p} = \frac{T}{n}$ is the unique maximum. \\

By LLN, 
\[
\hat{p} \overset{p}{\longrightarrow} p
\]

By CLT, 
\[
    \sqrt{n} \left( \hat{p} - p\right)  \overset{d}{\longrightarrow}  N(0, p(1-p))
\]
\end{example}

\begin{example}
(Normal): $X_1, \hdots X_n$ iid $N(\mu, \sigma^2)$ 

\begin{align*}
    L(\mu, \sigma^2 | X_1, \hdots X_n) &= \prod_{i = 1}^n f_{\mu, \sigma^2} (X_i) \\
    &= \prod_{i = 1}^n \left( \frac{1}{\sqrt{2 \pi \sigma^2}} \right) \exp \left( - \frac{1}{2\sigma^2}(X_i - \mu)^2 \right)  \\
    &= \left( \frac{1}{\sqrt{2 \pi \sigma^2}} \right)^n \exp \left( - \frac{1}{2\sigma^2} \sum\limits_{i = 1}^{n} (X_i - \mu)^2 \right)  \\
\end{align*}

The log likelihood is 
\begin{align*}
    l(\mu, \sigma^2 | X_1, \hdots X_n) &= \log \left( L(\mu, \sigma^2 | X_1, \hdots X_n) \right)  \\
    &= n \log \frac{1}{ \sqrt{2 \pi \sigma^2}} - \frac{1}{2 \sigma^2} \sum\limits_{i = 1}^{n}  \left( X_i - \mu \right)^2  \\
    &= - \frac{n}{2} \log 2 \pi \sigma^2 - \frac{n}{2} \log \sigma^2 - \frac{1}{2 \sigma^2} \sum\limits_{i = 1}^{n}  \left( X_i - \mu \right)^2
\end{align*}

Define
\[
    g(\mu, \sigma^2) := -\frac{n}{2} \log \sigma^2 - \frac{1}{2 \sigma^2} \sum\limits_{i = 1}^{n}  (X_i - \mu)^2 
\]

Setting first order partials to zero, we get 
\begin{align*}
    \frac{\partial g(\mu, \sigma^2)}{\partial \mu} &= \frac{1}{\sigma^2} \sum\limits_{i = 1}^{n} (X_i- \mu) = 0 \\
    \frac{\partial g(\mu, \sigma^2)}{\partial \sigma^2} &= - \frac{n}{2} \left( \frac{1}{\sigma^2} \right) + \frac{1}{2 \sigma^4} \sum\limits_{i = 1}^{n}  (X_i - \mu)^2 = 0
\end{align*}

Hence 
\[
    \hat{\mu}_{MLE} = \frac{1}{n}\sum\limits_{i = 1}^{n}  X_i = \bar{X} 
\]
\[
    \hat{\sigma}^2_{MLE} = \frac{1}{n} \sum\limits_{i = 1}^{n}  \left( X_i - \bar{X}  \right)^2
\]

To show that this is the global max, we can either 
\begin{itemize}
    \item Find the Hessian and show that it is negative definite, i.e. the function is concave 
    \item Argue that the derivative is 0 at only one point, and show that for extreme values of $\mu$ and $\sigma^2$ (i.e. $\mu \to -\infty$, $\mu \to \infty$, $\sigma^2 \to 0$, $\sigma^2 \to \infty$)
    \[
        l(\mu, \sigma^2) \to - \infty
    \]
\end{itemize} 

\textbf{Note}
\begin{enumerate}
    \item $\hat{\mu} \to \mu$ by LLN
    \item $ \sqrt{n} \left( \hat{\mu} - \mu \right) \overset{d}{=} N(0, \sigma^2)$
    \item $\hat{\sigma}^2 \overset{p}{\longrightarrow} \sigma^2$ by computing MSE. 
    \item $\sqrt{n} \left( \hat{\sigma}^2 - \sigma^2 \right) \overset{d}{\longrightarrow} N(0, 2 \sigma^4)$
\end{enumerate} 

\textbf{Proof of 4}: 
\begin{align*}
    \sqrt{n} \left( \hat{\sigma}^2 - \sigma^2 \right)  &= \sqrt{n} \left( \frac{\sigma^2}{n-1} \chi_{n - 1}^2 - \sigma^2 \right)  \\
    &= \sigma^2 \sqrt{n} \left( \frac{\chi_{n-1}^2}{n-1} - 1 \right)
\end{align*}

Note that 
\[
    \sqrt{n} \left( \frac{\chi_{n}^2}{n} - 1\right) \overset{d}{\longrightarrow}  N(0, 2)
\]

Hence 
\[
    \sqrt{n} \left( \hat{\sigma}^2 - \sigma^2 \right)  \overset{d}{\longrightarrow}  N(0, 2\sigma^4)
\]
\end{example}

\textcolor{red}{REVISIT}: check with the professor regarding this proof on asymptotic normality of $\hat{\sigma}^2_{MLE}$ without using Fisher Information.

\newpage












