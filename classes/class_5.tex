\chapter{Class 5}

\section{Chi-squared distribution}

Note that $\chi_{d}^2$ is the sum of $d$ iid standard normals squared. 
\[
  Z_1, \hdots Z_d \sim N(0, 1), \quad \sum\limits_{i=1}^{d} Z_i \sim \chi_d^2
\]

The \textit{chi-squared} distribution has expectation
\begin{align*}
    \mathbb{E}\left[ \chi_d^2\right] &= d \mathbb{E}\left[ Z_1^2\right]  \\
    &= d \quad
\end{align*}
Since 
\[
  \mathbb{E}\left[ Z_1^2\right]  = Var(Z_1) - \mathbb{E}\left[ Z_1\right]^2
\]

The \textit{chi-squared} distribution has variance 
\begin{align*}
    Var(\chi_d^2) &= Var \left( \sum\limits_{i = 1}^{d} Z_i^2 \right)  \\
    &= \sum\limits_{i = 1}^{d} Var(Z_i^2) \\
    &= 2d
\end{align*}

Since 
\begin{align*}
    Var(Z_1^2) &= \mathbb{E}\left[ Z_1^4\right]  - \mathbb{E}\left[ Z_1^2\right]^2 \\
    &= \mathbb{E}\left[ Z_1^4\right]  - 1 \\
    &= 3 - 1 \\
    &= 2
\end{align*}

\section{Sample Variance, cont'd}

\textbf{Result}: the sample variance follows a \textit{chi-squared} distribution. 
\[
  s^2 \sim \frac{\sigma^2}{n-1}\chi_{n-1}^2
\]

\textbf{Proof}: 

Consider $\vtr{Y} = \begin{pmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{pmatrix} $ = $ \begin{pmatrix} X_1 - \bar{X}  \\ X_2 - \bar{X}  \\\vdots \\ X_n - \bar{X} \end{pmatrix} $. We express $\vtr{Y} = \mtr{A} \vtr{X}$ for some matrix $\mtr{A}$.
\[
\vtr{Y} = \begin{pmatrix} 
  1 - \frac{1}{n} & - \frac{1}{n} & \hdots & - \frac{1}{n}   \\
   - \frac{1}{n} & 1 - \frac{1}{n} & \hdots & - \frac{1}{n}   \\
  \vdots & \vdots & \vdots & \vdots \\
   - \frac{1}{n} & - \frac{1}{n} & \hdots & 1 - \frac{1}{n}   \\
\end{pmatrix} \vtr{X}
\]

Since 
\[
  \vtr{X} \sim N_n \left( \mu \vtr{1}, \sigma^2 \mtr{I} \right) 
\]

Hence 
\begin{align*}
  \vtr{Y} &\sim N_n \left( \mtr{A} \vtr{\mu}, \sigma^2 \mtr{A}\mtr{A}^T \right)   \\
  &= N_n( \mu \mtr{A}\vtr{1}, \sigma^2 \mtr{A} \mtr{A}^T) \\
  &= N_n( \vtr{0}, \sigma^2 \mtr{A} \mtr{A}^T)  \text{ since } \mtr{A} \vtr{1} = \vtr{0}\\
  &= N_n(\vtr{0}, \sigma^2 \mtr{A}^2) \text{ since } \mtr{A} \text{ symmetric}
\end{align*}

Note that \[
  \mtr{A} = \mtr{I} - \frac{1}{n} \vtr{1} \vtr{1}^T
\]

Where $\vtr{1} \vtr{1}^T$ is a rank 1 symmetric matrix. Hence $\vtr{1}\vtr{1}^T$ has at most 1 non-zero eigenvalue. Since the sum of eigenvalues is the trace, and $tr(\vtr{1}\vtr{1}^T) = n$, the non-zero eigenvalue is $n$. $\vtr{1} \vtr{1}^T$ has eigenvalues $(n, 0, 0, \hdots, 0)$. $ \frac{1}{n}\vtr{1} \vtr{1}^T$  has eigenvalues $(1, 0, 0, \hdots)$. \\

Therefore, $\mtr{I} - \frac{1}{n}\vtr{1} \vtr{1}^n$ has eigenvalues $(1, 1, \hdots, 1, 0)$ (This only works because of $\mtr{I}$).  \\

We can express $s^2$ in terms of $Y$. 
\[
  s^2 = \frac{1}{n-1} \sum\limits_{i=1}^{n}  (X_i - \bar{X} )^2 = \frac{1}{n-1} \vtr{Y}^T \vtr{Y}
\]
It suffices to show that 
\[
  \vtr{Y}^T \vtr{Y} \sim \sigma^2 \chi_{n-1}^2
\]

\textbf{Note}: $Y$ has elements that are normal, i.e. $Y_i = X_i - \bar{X} $ is a difference of normals. However, $Y_i$ is not independent due to $\bar{X} $. \\

\textbf{Fact}: Denote $\mtr{\Sigma} = \mtr{A}^2$, and we define $\vtr{Z} = N_n( \vtr{0}, \mtr{I})$. Then, 
\[
  Y \overset{d}{=} \sigma \mtr{\Sigma}^{ \frac{1}{2}} \vtr{Z} \sim N_n ( \vtr{0}, \sigma^2 \mtr{A})
\]

This is true because
\begin{align*}
    \sigma \mtr{\Sigma}^T \mtr{\Sigma} &\sim N_n \left( 
        \vtr{0}, 
        \sigma^2 
            \left(\mtr{\Sigma}^{\frac{1}{2}}\right)^T \mtr{\Sigma}^{\frac{1}{2}} 
    \right) \\
    &= N_n \left( \vtr{0}, \sigma^2 \mtr{A}^T \mtr{A} \right)  \\
    &= N_n \left( \vtr{0}, \sigma^2 \mtr{A}^2 \right) 
\end{align*}

Hence
\begin{align*}
    \vtr{Y}^T \vtr{Y} &= \sigma^2 \vtr{Z}^T \left( \mtr{\Sigma}^{ \frac{1}{2}} \right)^T \mtr{\Sigma}^{ \frac{1}{2}} \vtr{Z} \\
    &= \sigma^2 \vtr{Z} \mtr{A}^T \mtr{A} \vtr{Z} \\
    &= \sigma^2 \vtr{Z}^T \mtr{A} \vtr{Z}
\end{align*}

Hence it suffices to show that 
\[
    \vtr{Z}^T \mtr{A} \vtr{Z} \sim \chi_{n - 1}^2
\]

Note that if $\mtr{A} = \mtr{I}$, then $ \vtr{Z}^T \vtr{Z} \sim \chi_n^2$. \\

By spectral decomposition, 
\[
  \vtr{Z}\mtr{A}^T \vtr{Z} = \vtr{Z} \vtr{P} \mtr{\Lambda} \vtr{P}^T \vtr{Z}
\]

Denote $\vtr{P}^T \vtr{Z} = \vtr{W} \in \mathbb{R}^n$. 
\[
  \vtr{Z}\mtr{A}^T \vtr{Z} = \vtr{Z} \vtr{P} \mtr{\Lambda} \vtr{P}^T \vtr{Z} = \vtr{W}^T \vtr{\Lambda} \vtr{W}
\]

Because of spectral decomposition, we know that $\vtr{P} and \mtr{P}^T$ are orthogonal matrices whose product is the identity. Applying an orthogonal matrix to a multivariate standard normal, i.g. $\mtr{A} \vtr{Z}$, does not change the multivariate standard normal distribution. 
\[
  \vtr{W} \sim N_n ( \vtr{0}, \vtr{P} \vtr{P}^T) = N_n( \vtr{0}, \mtr{I})
\]

Hence 
\begin{align*}
  \vtr{W}^T \mtr{\Lambda} \vtr{W}  &= \vtr{W}^T \begin{pmatrix} 
    1 & 0 & \hdots & 0 & 0 \\  
    0 & 1 & \hdots & 0 & 0 \\  
    \vdots & \vdots & \vdots & \vdots & 0 \\  
    0 & 0 & \hdots & 1 & 0  \\  
    0 & 0 & \hdots & 0 & 0 
  \end{pmatrix}  \vtr{W} \\
  &= \sum\limits_{i = 1}^{n-1} W_i^2 \\
  &\sim \chi_{n - 1}^2 \qed
\end{align*}

\section{Joint distribution of sample mean and sample variance}

We know the marginal distributions for the sample mean and variance 
\[
  \bar{X}  sim N \left( \mu, \frac{\sigma^2}{n} \right) , s^2 \sim \frac{\sigma^2}{n - 1} \chi_{n-1}^2
\]

\textbf{Result}: If $X_1, X_2, \hdots X_n$ iid normal, $\bar{X} , s^2$ independent. 

\textbf{Proof}: Define $\vtr{Z} = \begin{pmatrix} X_1 - \bar{X} \\ X_2 - \bar{X} \\ \vdots \\ X_n - \bar{X} \\ \bar{X}  \end{pmatrix} $
\[
  \vtr{Z} = \mtr{B} \vtr{X}, \mtr{B} \in \mathbb{R}^{(n + 1) \times n}, \mtr{B} = \begin{pmatrix} 
    & & & & \\  
    & & & & \\  
    & & \mtr{A} & & \\  
    & & & & \\  
    & & & & \\  
    \frac{1}{n} & \frac{1}{n} & \hdots & \frac{1}{n}
  \end{pmatrix}
\]

Hence $\vtr{Z}$ is $(n+1)$-dimensional normal.  \\

If we show that $X_1 - \bar{X}   \perp \bar{X} $, $X_2 - \bar{X}  \perp \bar{X} \hdots X_n - \bar{X}  \perp \bar{X} $, then 
\[
  \begin{pmatrix} 
    X_1 - \bar{X}  \\  
    X_2 - \bar{X}  \\  
    \vdots
    X_n - \bar{X}  \\  
  \end{pmatrix} \perp \bar{X}  \implies s^2 \perp \bar{X} 
\]

Since $\vtr{Z} = \mtr{B} \vtr{X}$ is multivariate normal, it suffices to check that the covariance is 0. 

\begin{framed}
    \textbf{Note}: As a general strategy, to show independence, we can show in two steps
    \begin{enumerate}
        \item show jointly normal
        \item show 0 covariance
    \end{enumerate}
\end{framed} 

It suffices to show $Cov( \bar{X} , X_i - \bar{X} ) = 0$ for all $1 \leq i \leq n$. 

Note that 
\[
  \bar{X}  = \frac{1}{n} \sum\limits_{i = 1}^{n} X_i = \sum\limits_{i = 1}^{n} a_i X_i, \quad a_1 = a_2 = \hdots a_n = \frac{1}{n}
\]

\[
  X_1 - \bar{X}  = \sum\limits_{i = 1}^{n} b_i X_i, \quad b_1 = 1 - \frac{1}{n}, b_2 = b_3 = \hdots b_n = -\frac{1}{n}
\]

\begin{align*}
    Cov \left( \sum\limits_{i=1}^{n} a_i X_i , \sum\limits_{i = 1}^{n } b_i X_i\right)  &= \sum\limits_{i = 1}^{n} \sum\limits_{j = 1}^{n}  a_i b_j Cov(X_i X_j) \\
    &= \sum\limits_{i = 1}^{n}  a_i b_i Var(X_i) \\
    &= \sigma^2 \sum\limits_{i = 1}^{n } a_i b_i \\
    &= 0
\end{align*}































